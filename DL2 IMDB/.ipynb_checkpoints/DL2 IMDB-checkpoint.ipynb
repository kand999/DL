{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178cb823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "imdb=pd.read_csv(\"IMDB Dataset.csv\")\n",
    "imdb\n",
    "\n",
    "#Sentiment Analysis\n",
    "\n",
    "counts = imdb['sentiment'].value_count().plot.bar(color=['blue', 'red'])\n",
    "plt.title('Sentiment Counts')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "#1st method to convert categorical values into numerical values\n",
    "imdb_data=imdb\n",
    "imdb['sentiment']=imdb_data['sentiment'].replace({\"positive\":1,\"negative\":0})\n",
    "imdb_data\n",
    "\n",
    "#2nd method for converting categorical values to numerical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lable=LabelEncoder()\n",
    "imdb['sentiment']=lable.fit_transform(imdb['sentiment'])\n",
    "imdb\n",
    "\n",
    "#Preprocessing\n",
    "imdb['review']=imdb['review'].str.lower()\n",
    "imdb\n",
    "\n",
    "import string \n",
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "exclude_punc=set(string.punctuation)\n",
    "stop_words=set(stopwords.words(\"English\"))\n",
    "#removing punctuations and stopwords\n",
    "def cleaning_text(text):\n",
    "    tokens=word_tokenize(text)\n",
    "    filtered_text=[word for word in tokens if word.lower() not in stop_words and word not in exclude_punc]\n",
    "    return ' '.join(filtered_text)\n",
    "imdb['review']=imdb['review'].apply(cleaning_text)\n",
    "imdb.head(10)\n",
    "\n",
    "imdb['words_in_sent']=imdb['review'].apply(lambda x:str(x).split())\n",
    "\n",
    "pos_sent=imdb[imdb['sentiment']==1]\n",
    "neg_sent=imdb[imdb['sentiment']==0]\n",
    "pos_sent\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "nltk.download('opinion_lexicon')\n",
    "\n",
    "from nltk.corpus import opinion_lexicon\n",
    "\n",
    "pos_list=set(opinion_lexicon.positive())\n",
    "neg_list=set(opinion_lexicon.negative())\n",
    "pos_list=list(pos_list)\n",
    "neg_list=list(neg_list)\n",
    "pos_list\n",
    "\n",
    "common_posi_words=[]\n",
    "for word in pos_sent['words_in_sent']:\n",
    "    for w in word:\n",
    "        if w in pos_list:\n",
    "            common_posi_words.append(w)\n",
    "common_posi_words     \n",
    "        \n",
    "common_neg_words=[]\n",
    "for word in neg_sent['words_in_sent']:\n",
    "    for w in word:\n",
    "        if w in neg_list:\n",
    "            common_neg_words.append(w)\n",
    "common_neg_words\n",
    "        \n",
    "#retireving +ve words with counts\n",
    "common_posi_words=Counter(common_posi_words)\n",
    "most_common_posi_words=pd.DataFrame(common_posi_words.most_common(50))\n",
    "most_common_posi_words.columns=['Pos_common_words','count']\n",
    "most_common_posi_words\n",
    "\n",
    "common_nega_words=Counter(common_neg_words)\n",
    "most_common_neg_words=pd.DataFrame(common_nega_words.most_common(50))\n",
    "most_common_neg_words.columns=['Neg_common_words','count']\n",
    "most_common_neg_words\n",
    "\n",
    "#Common +ve and -ve words analysis\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(most_common_posi_words, x=\"count\", y=\"Pos_common_words\", title='Most Commmon Positive Words', orientation='h', \n",
    "             width=700, height=700,color='Pos_common_words')\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig = px.bar(most_common_neg_words, x=\"count\", y=\"Neg_common_words\", title='Most Commmon Negative Words', orientation='h', \n",
    "             width=700, height=700,color='Neg_common_words')\n",
    "fig.show()\n",
    "\n",
    "#Common +ve and -ve words analysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(#max_features = 20000,\n",
    "                            min_df=2, \n",
    "                            max_df=0.8, \n",
    "                            max_features=5000, \n",
    "                            sublinear_tf=True, \n",
    "                            use_idf=True, \n",
    "                            stop_words='english')\n",
    "imdb_vector = vectorizer.fit_transform(imdb['review'])\n",
    "\n",
    "\n",
    "X=imdb['review']\n",
    "y=imdb['sentiment']\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,stratify=y,random_state=50)  \n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "X_train\n",
    "\n",
    "X_test\n",
    "\n",
    "y_train\n",
    "\n",
    "y_test\n",
    "\n",
    "vectorizer=TfidfVectorizer(max_features=10000,ngram_range=(1,3))\n",
    "X_train_tfidf=vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf=vectorizer.fit_transform(X_test)\n",
    "print(X_train_tfidf.shape)\n",
    "print(X_test_tfidf.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "#Neural Network Modeling\n",
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "\n",
    "\n",
    "#NN model\n",
    "model=Sequential()\n",
    "model.add(Dense(20,input_shape=(X_train_tfidf.shape[1],), activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "history=model.fit(X_train_tfidf.toarray(),y_train,\n",
    "                  epochs=20,verbose=1,\n",
    "                  validation_data=(X_test_tfidf.toarray(),y_test))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.title('Accuracy Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "model.evaluate(X_test_tfidf.toarray(),y_test,verbose=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
